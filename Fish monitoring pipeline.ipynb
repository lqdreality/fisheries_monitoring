{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Fish monitoring pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some setup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from util.data_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "new_style = {'grid': False}\n",
    "plt.rc('axes', **new_style)\n",
    "p = sns.color_palette()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load localizer train dataset. This dataset comprises of images taken from different boat decks with fishes of different species. They are annotated with the position (x, y, w, h) where each fish is located in each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Localizer train dataset shape: (4371, 224, 224, 3)\n",
      "Localizer annotated boxes shape: (4371, 4)\n"
     ]
    }
   ],
   "source": [
    "boxes = pd.read_csv('/a/data/fisheries_monitoring/data/detection_labels/boxes.csv',\n",
    "                     names = [\"img\", \"x\",\"y\",\"w\",\"h\"])\n",
    " \n",
    "INPUT_LOC_WIDTH = 224\n",
    "INPUT_LOC_HEIGHT = 224\n",
    "PATH = '/a/data/fisheries_monitoring/data/localizers/original/'\n",
    "loc_data, loc_target, loc_index = load_raw_data(boxes, INPUT_LOC_WIDTH, INPUT_LOC_HEIGHT, PATH)\n",
    "#loc_res_data, loc_res_target, loc_res_index = resize_data_and_boxes(loc_data, loc_target, loc_index,\n",
    "                                                                    \n",
    "print \"Localizer train dataset shape:\", loc_data.shape\n",
    "print \"Localizer annotated boxes shape:\", loc_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load classifier labels and train dataset. This dataset is build by cropped images from the original dataset in which the fish ocuppies most part of the image. The labels are the specie of each fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "Load folder ALB (Index: 0)\n",
      "Load folder BET (Index: 1)\n",
      "Load folder DOL (Index: 2)\n",
      "Load folder LAG (Index: 3)\n",
      "Load folder OTHER (Index: 4)\n",
      "Load folder SHARK (Index: 5)\n",
      "Load folder YFT (Index: 6)\n",
      "Load folder NoF (Index: 7)\n",
      "Read train data time: 48.43 seconds\n",
      "Convert to numpy...\n",
      "Convert to float...\n",
      "Train shape: (4836, 224, 224, 3)\n",
      "4836 train samples\n"
     ]
    }
   ],
   "source": [
    "INPUT_CLS_WIDTH = 224\n",
    "INPUT_CLS_HEIGHT = 224\n",
    "PATH = '/a/data/fisheries_monitoring/data/classifiers/cropped_from_origin'\n",
    "\n",
    "train_data, train_target, train_id = load_cropped_train(PATH, INPUT_CLS_WIDTH, INPUT_CLS_HEIGHT)\n",
    "\n",
    "print 'Convert to numpy...'\n",
    "train_data = np.array(train_data)\n",
    "train_target = np.array(train_target)\n",
    "\n",
    "print 'Convert to float...'\n",
    "train_data = train_data.astype('float32')\n",
    "cropped_data = train_data / 255\n",
    "labels = np_utils.to_categorical(train_target, 8)\n",
    "\n",
    "print 'Train shape:', cropped_data.shape\n",
    "print cropped_data.shape[0], 'train samples'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create pipeline with desired localizer and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from models.pipeline import Pipeline\n",
    "import models.classifiers as cls\n",
    "import models.localizers as loc\n",
    "\n",
    "model = Pipeline(loc.ResNet50(), cls.ResNet50())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Split localizer dataset and set it into the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_loc_train, X_loc_test, y_loc_train, y_loc_test = train_test_split(loc_data, loc_target,\n",
    "                                                                    test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.set_localizer_train_data(X_loc_train, y_loc_train, X_loc_test, y_loc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3496, 224, 224, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_loc_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Do the same for the classifier dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_cls_train, X_cls_test, y_cls_train, y_cls_test = train_test_split(cropped_data, labels,\n",
    "                                                                    test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.set_classifier_train_data(X_cls_train, y_cls_train, X_cls_test, y_cls_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3496 samples, validate on 875 samples\n",
      "Epoch 1/10\n",
      "69s - loss: 14136.0497 - val_loss: 14660.9348\n",
      "Epoch 2/10\n",
      "67s - loss: 13503.2128 - val_loss: 14022.3538\n",
      "Epoch 3/10\n",
      "67s - loss: 12758.4529 - val_loss: 13374.5024\n",
      "Epoch 4/10\n",
      "67s - loss: 12295.8880 - val_loss: 13128.7311\n",
      "Epoch 5/10\n",
      "67s - loss: 11959.6909 - val_loss: 12631.9379\n",
      "Epoch 6/10\n",
      "68s - loss: 11359.7295 - val_loss: 12369.1799\n",
      "Epoch 7/10\n",
      "67s - loss: 11060.6516 - val_loss: 12105.3462\n",
      "Epoch 8/10\n",
      "67s - loss: 10744.4295 - val_loss: 11858.9031\n",
      "Epoch 9/10\n",
      "66s - loss: 10371.2897 - val_loss: 11585.0026\n",
      "Epoch 10/10\n",
      "67s - loss: 10047.2955 - val_loss: 11302.5051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "models/pipeline.py:55: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(self.loc_data['X_val'], self.loc_data['y_val']))\n",
      "models/pipeline.py:61: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(self.cls_data['X_val'], self.cls_data['y_val']))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3868 samples, validate on 968 samples\n",
      "Epoch 1/10\n",
      "58s - loss: 0.7915 - val_loss: 2.8573\n",
      "Epoch 2/10\n",
      "56s - loss: 0.7069 - val_loss: 3.5181\n",
      "Epoch 3/10\n",
      "56s - loss: 0.6351 - val_loss: 3.9849\n",
      "Epoch 4/10\n",
      "56s - loss: 0.5864 - val_loss: 4.4043\n",
      "Epoch 5/10\n",
      "57s - loss: 0.5340 - val_loss: 4.9398\n",
      "Epoch 6/10\n",
      "56s - loss: 0.5008 - val_loss: 5.4129\n",
      "Epoch 7/10\n",
      "57s - loss: 0.4712 - val_loss: 5.7817\n",
      "Epoch 8/10\n",
      "57s - loss: 0.4428 - val_loss: 5.9536\n",
      "Epoch 9/10\n",
      "58s - loss: 0.4187 - val_loss: 6.1860\n",
      "Epoch 10/10\n",
      "57s - loss: 0.3927 - val_loss: 6.5240\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "nb_epoch = 10\n",
    "random_state = 51\n",
    "\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968/968 [==============================] - 11s    \n",
      "(968, 224, 224, 3)\n",
      "968/968 [==============================] - 15s    \n",
      "log loss score:  16.6984993328\n"
     ]
    }
   ],
   "source": [
    "# TODO: We are using for testing the same dataset as for validation, need to change it\n",
    "predictions = model.predict(X_cls_test.astype('float32'), batch_size=batch_size)\n",
    "# Compute loss\n",
    "score = log_loss(y_cls_test, predictions)\n",
    "print \"log loss score: \", score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.51652892562\n"
     ]
    }
   ],
   "source": [
    "acc = (np.argmax(predictions, 1) == np.argmax(y_cls_test, 1)).mean()\n",
    "\n",
    "print \"accuracy: \", acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
